---
output:
  pdf_document: default
  html_document: default
---
Data Generation process for our models

```{r}
set.seed(2020)

n <- 250  # sample size

x1 <- rnorm(n)
x2 <- rnorm(n)
x3 <- rnorm(n)
x4 <- rbinom(n, 1, 0.5)
x5 <- sample(1:3, n, replace = TRUE)
```

```{r}
#g function like they describe
g_func <- function(x) {
  return(ifelse(x == 1, 2, ifelse(x == 2, -1, -4)))
}
```

```{r}
#Get mu
mu_linear <- 1 + g_func(x4) + x1 * x3
mu_nonlinear <- -6 + g_func(x4) + 6 * abs(x3 - 1)
```

```{r}
#Get tau
tau_homogeneous <- rep(3, n)
tau_heterogeneous <- 1 + 2 * x2 * x5
```

```{r}
#Get the propensity score
ui <- runif(n)
s_linear <- sd(mu_linear)
s_nonlinear <- sd(mu_nonlinear)

# Standard normal CDF
phi <- pnorm

# Propensity scores
pi_linear <- 0.8 * phi(3 * mu_linear / s_linear - 0.5 * x1) + 0.05 + ui / 10
pi_nonlinear <- 0.8 * phi(3 * mu_nonlinear / s_nonlinear - 0.5 * x1) + 0.05 + ui / 10
```

```{r}
#Simulate Treatment assignments
Z_linear <- rbinom(n, 1, pi_linear)
Z_nonlinear <- rbinom(n, 1, pi_nonlinear)

# error term
epsilon <- rnorm(n)
```

```{r}
#Now lets get our outcomes according to this formula: Y = mu + tau * Z + rnorm(n)
#Create the 4 datasets
#Linear and homogeneous
Y_lin_hom <- mu_linear + tau_homogeneous * Z_linear + epsilon

#Linear heterogeneous
Y_lin_het <- mu_linear + tau_heterogeneous * Z_linear + epsilon

#Non-linear homogeneous
Y_nonlin_hom <- mu_nonlinear + tau_homogeneous * Z_nonlinear + epsilon

#Non-linear heterogeneous
Y_nonlin_het <- mu_nonlinear + tau_heterogeneous * Z_nonlinear + epsilon
```

```{r}
#Throw these bad boys into a dataframes
data_lin_hom <- data.frame(Y = Y_lin_hom, Z = Z_linear, x1, x2, x3, x4, x5)
data_lin_het <- data.frame(Y = Y_lin_het, Z = Z_linear, x1, x2, x3, x4, x5)
data_nonlin_hom <- data.frame(Y = Y_nonlin_hom, Z = Z_nonlinear, x1, x2, x3, x4, x5)
data_nonlin_het <- data.frame(Y = Y_nonlin_het, Z = Z_nonlinear, x1, x2, x3, x4, x5)
```

```{r}
#Check those dataframes out
print(data_lin_hom)
print(data_lin_het)
print(data_nonlin_hom)
print(data_nonlin_het)
```

```{r}
library(bcf)
library(grf)
library(MASS)
library(mvtnorm)
library(dplyr)
library(doParallel)
library(foreach)

set.seed(2020)
n_sim <- 50
n <- 250
p <- 5
results_file <- "bcf_simulation_results.csv"

if (file.exists(results_file)) file.remove(results_file)

#g(x4) mapping for categorical variable
g_fun <- function(x4) {
  vals <- c(2, -1, -4)
  return(vals[x4])
}

#treatment effect function
tau_fun <- list(
  homogeneous = function(x) rep(3, nrow(x)),
  heterogeneous = function(x) 1 + 2 * x[,2] * x[,5]
)

#prognostic function
mu_fun <- list(
  linear = function(x) 1 + g_fun(x[,4]) + x[,1] * x[,3],
  nonlinear = function(x) -6 + g_fun(x[,4]) + 6 * abs(x[,3] - 1)
)

#propensity function
propensity_score <- function(mu, x1) {
  s <- sd(mu)
  prob <- 0.8 * pnorm(3 * mu/s - 0.5 * x1) + 0.05 + runif(length(mu)) / 10
  return(pmin(pmax(prob, 0.01), 0.99))
}

#estimation metrics
compute_metrics <- function(estimates, true_values, ci_lower, ci_upper) {
  rmse <- sqrt(mean((estimates - true_values)^2))
  coverage <- mean(ci_lower <= true_values & true_values <= ci_upper)
  len <- mean(ci_upper - ci_lower)
  return(c(rmse=rmse, coverage=coverage, len=len))
}

#PARALLELIZATION. USE IN COMPUTE CLUSTER WITH MULTIPLE CORES IF POSSIBLE. 
#note: 48 cores and 256gb memory takes ~11 hours in 'Oscar' to run

#detects available cores (adjust as needed for your cluster setup)
n_cores <- min(parallel::detectCores(), 48)  # Use up to 48 cores
cat(sprintf("Setting up parallel processing with %d cores\n", n_cores))

#creates and registers cluster - IMPORTANT: must be done before starting the loop
cl <- makeCluster(n_cores)

#exports necessary functions and variables to all worker nodes
clusterExport(cl, c("g_fun", "tau_fun", "mu_fun", "propensity_score", "compute_metrics"))

#loads required packages on all nodes
clusterEvalQ(cl, {
  library(bcf)
  library(MASS)
  library(mvtnorm)
  library(dplyr)
})

#registers the parallel backend
registerDoParallel(cl)

cat("Starting parallel simulations...\n")

#creates a dataframe with all simulation configurations
sim_grid <- expand.grid(
  sim = 1:n_sim,
  mu_type = names(mu_fun),
  tau_type = names(tau_fun),
  stringsAsFactors = FALSE
)


results <- foreach(i = 1:nrow(sim_grid), .combine = rbind, 
                   .packages = c("bcf", "MASS", "mvtnorm", "dplyr")) %dopar% {
                     
     current_sim <- sim_grid[i, "sim"]
     current_mu_type <- as.character(sim_grid[i, "mu_type"])
     current_tau_type <- as.character(sim_grid[i, "tau_type"])
     
     cat(sprintf("Running simulation %d: mu=%s, tau=%s\n", 
                 current_sim, current_mu_type, current_tau_type))

      #generate X
      x1 <- rnorm(n)
      x2 <- rnorm(n)
      x3 <- rnorm(n)
      x4 <- sample(1:3, n, replace = TRUE)
      x5 <- rbinom(n, 1, 0.5)
      X <- cbind(x1, x2, x3, x4, x5)
      
      x4_oh <- model.matrix(~ as.factor(x4) - 1)
      X_model <- cbind(x1, x2, x3, x5, x4_oh)
      
      #accesses functions through explicit namespace to avoid scope issues
      #gets the correct function from mu_fun and tau_fun
      mu_function <- mu_fun[[current_mu_type]]
      tau_function <- tau_fun[[current_tau_type]]
      
      mu <- mu_function(X)
      tau <- tau_function(X)
      pi <- propensity_score(mu, x1)
      z <- rbinom(n, 1, pi)
      y <- mu + tau * z + rnorm(n)
      
      fit_bcf <- bcf(
        y = y,
        z = z,
        x_control = X_model,
        x_moderate = X_model,
        pihat = pi,
        nburn = 1000,
        nsim = 2000,
        verbose = FALSE
      )
      
      tau_post <- fit_bcf$tau
      est_ate <- rowMeans(tau_post)
      est_ate_mean <- mean(est_ate)
      est_ate_ci <- quantile(est_ate, probs = c(0.025, 0.975))
      
      true_ate <- mean(tau)
      ate_metrics <- compute_metrics(est_ate_mean, true_ate, est_ate_ci[1], est_ate_ci[2])
      
      est_cate_mean <- colMeans(tau_post)
      est_cate_ci_lwr <- apply(tau_post, 2, quantile, 0.025)
      est_cate_ci_upr <- apply(tau_post, 2, quantile, 0.975)
      cate_metrics <- compute_metrics(est_cate_mean, tau, est_cate_ci_lwr, est_cate_ci_upr)
      
      data.frame(
        method = "bcf",
        dgp_mu = current_mu_type,
        dgp_tau = current_tau_type,
        sim = current_sim,
        ate_rmse = ate_metrics["rmse"],
        ate_cover = ate_metrics["coverage"],
        ate_len = ate_metrics["len"],
        cate_rmse = cate_metrics["rmse"],
        cate_cover = cate_metrics["coverage"],
        cate_len = cate_metrics["len"]
      )
 }
      
      
write.csv(results, file = results_file, row.names = FALSE)

#stops the cluster when done
stopCluster(cl)
cat("All simulations completed!\n")
```

```{r}
library(dbarts)

set.seed(2020)

# List of datasets
data_list <- list(
  lin_hom = data_lin_hom,
  lin_het = data_lin_het,
  nonlin_hom = data_nonlin_hom,
  nonlin_het = data_nonlin_het
)

# True CATEs for each dataset
true_tau_list <- list(
  lin_hom = tau_homogeneous,
  lin_het = tau_heterogeneous,
  nonlin_hom = tau_homogeneous,
  nonlin_het = tau_heterogeneous
)

results <- data.frame(
  dataset = character(),
  ATE_RMSE = numeric(),
  CATE_RMSE = numeric()
)

for (data_name in names(data_list)) {
  data <- data_list[[data_name]]
  true_tau <- true_tau_list[[data_name]]
  true_ATE <- mean(true_tau)
  n <- nrow(data)
  X <- data[, c("x1", "x2", "x3", "x4", "x5")]
  Z <- data$Z
  mu <- data$Y - true_tau * Z  # to regenerate Y with noise later

  # Storage
  ate_rmse <- numeric(50)
  cate_rmse <- numeric(50)

  for (i in 1:50) {
    epsilon <- rnorm(n)
    Y_sim <- mu + true_tau * Z + epsilon

    # Fit BART separately for Z=1 and Z=0
    X_treated <- X[Z == 1, ]
    Y_treated <- Y_sim[Z == 1]
    X_control <- X[Z == 0, ]
    Y_control <- Y_sim[Z == 0]

    bart_fit_treat <- dbarts::bart(
      x.train = as.matrix(X_treated),
      y.train = Y_treated,
      x.test = as.matrix(X),
      verbose = FALSE
    )

    bart_fit_control <- dbarts::bart(
      x.train = as.matrix(X_control),
      y.train = Y_control,
      x.test = as.matrix(X),
      verbose = FALSE
    )

    mu1_hat <- colMeans(bart_fit_treat$yhat.test)
    mu0_hat <- colMeans(bart_fit_control$yhat.test)

    cate_hat <- mu1_hat - mu0_hat
    ate_hat <- mean(cate_hat)

    # Store RMSEs
    ate_rmse[i] <- sqrt((ate_hat - true_ATE)^2)
    cate_rmse[i] <- sqrt(mean((cate_hat - true_tau)^2))
  }

  # Save results
  results <- rbind(results, data.frame(
    dataset = data_name,
    ATE_RMSE = mean(ate_rmse),
    CATE_RMSE = mean(cate_rmse)
  ))
}

print(results)
```

```{r}
library(glmnet)

set.seed(2020)
simulations <- 50

# --- assume you've already defined data_list and true_tau_list ---
lasso_results <- data.frame(
  dataset         = character(),
  ATE_RMSE        = numeric(),
  ATE_COVERAGE    = numeric(),
  ATE_CI_LENGTH   = numeric(),
  CATE_RMSE       = numeric(),
  CATE_COVERAGE   = numeric(),
  CATE_CI_LENGTH  = numeric(),
  stringsAsFactors = FALSE
)

for(data_name in names(data_list)) {
  data      <- data_list[[data_name]]
  true_tau  <- true_tau_list[[data_name]]
  true_ATE  <- mean(true_tau)
  n         <- nrow(data)

  # covariates + interactions
  X_conf <- model.matrix(~ x1 + x2 + x3 + x4 + x5, data)[,-1]
  Z_vec  <- data$Z
  X_int  <- X_conf * Z_vec
  colnames(X_int) <- paste0("Z.", colnames(X_conf))

  # design for glmnet
  D_mat <- cbind(Z = Z_vec, X_conf, X_int)
  pf    <- c(0, rep(1, ncol(X_conf) + ncol(X_int)))  # no penalty on Z

  # per‐simulation storage
  ate_hat    <- numeric(simulations)
  cate_rmse  <- numeric(simulations)
  ate_cov    <- logical(simulations)
  ate_len    <- numeric(simulations)
  cate_cov_m <- matrix(FALSE, nrow=simulations, ncol=n)
  cate_len_m <- matrix(0,     nrow=simulations, ncol=n)

  for(i in seq_len(simulations)) {
    # 1) re‐simulate outcome
    mu_vec <- data$Y - true_tau * Z_vec
    Y_sim  <- mu_vec + true_tau * Z_vec + rnorm(n)

    # 2) fit CV‐Lasso + final Lasso
    cvm <- cv.glmnet(D_mat, Y_sim, alpha = 1,
                     penalty.factor = pf, nfolds = 5)
    λmin <- cvm$lambda.min
    fit  <- glmnet(D_mat, Y_sim, alpha = 1,
                   lambda = λmin, penalty.factor = pf)

    # 3) post‐Lasso OLS on selected
    sel <- rownames(coef(fit))[coef(fit)[,1] != 0]
    sel <- setdiff(sel, "(Intercept)")
    if(!"Z" %in% sel) sel <- c("Z", sel)
    df_ols <- data.frame(Y = Y_sim, D_mat[, sel, drop=FALSE])
    form   <- as.formula(paste("Y ~", paste(names(df_ols)[-1], collapse = " + ")))
    ols    <- lm(form, data = df_ols)

    # 4) Extract ATE estimate and its SE from summary()
    sm    <- summary(ols)$coefficients
    est_z <- sm["Z", "Estimate"]
    se_z  <- sm["Z", "Std. Error"]
    zval  <- qnorm(0.975)
    lo_z  <- est_z - zval * se_z
    hi_z  <- est_z + zval * se_z

    ate_hat[i] <- est_z
    ate_cov[i] <- (lo_z <= true_ATE && true_ATE <= hi_z)
    ate_len[i] <- hi_z - lo_z

    # 5) form L matrix for each τ_i
    V    <- vcov(ols)
    coefs_nm <- names(coef(ols))
    p    <- length(coefs_nm)
    L    <- matrix(0, nrow=n, ncol=p, dimnames=list(NULL, coefs_nm))
    L[ , "Z"] <- 1
    for(j in colnames(X_int)) {
      if(j %in% coefs_nm) L[ , j] <- X_int[ , j]
    }

    tau_hat <- as.numeric(L %*% coef(ols))
    se_tau  <- sqrt(rowSums((L %*% V) * L))

    # 6) CATE metrics
    cate_rmse[i]        <- sqrt(mean((tau_hat - true_tau)^2))
    lo_tau             <- tau_hat - zval * se_tau
    hi_tau             <- tau_hat + zval * se_tau
    cate_cov_m[i, ]    <- (lo_tau <= true_tau & true_tau <= hi_tau)
    cate_len_m[i, ]    <- hi_tau - lo_tau
  }

  # summarize over sims
  lasso_results <- rbind(
    lasso_results,
    data.frame(
      dataset         = data_name,
      ATE_RMSE        = sqrt(mean((ate_hat - true_ATE)^2)),
      ATE_COVERAGE    = mean(ate_cov),
      ATE_CI_LENGTH   = mean(ate_len),
      CATE_RMSE       = mean(cate_rmse),
      CATE_COVERAGE   = mean(cate_cov_m),
      CATE_CI_LENGTH  = mean(cate_len_m),
      stringsAsFactors = FALSE
    )
  )
}

print(lasso_results)

```



 
